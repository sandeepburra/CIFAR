{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3555,
     "status": "ok",
     "timestamp": 1571800558932,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "wVIx_KIigxPV",
    "outputId": "09589814-fb74-4b9b-e97e-57ffc672e5d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "import tensorflow.python.keras\n",
    "from tensorflow.python.keras import models, layers\n",
    "from tensorflow.python.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "# import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "config.gpu_options.allow_growth= True\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "compression = 0.55\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5337,
     "status": "ok",
     "timestamp": 1571800644940,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "mB7o3zu1g6eT",
    "outputId": "65d8e8db-3692-4c70-8a15-f6a324bb1d84"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1571800647028,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "287_ZgFXB-ZX",
    "outputId": "3ba0af91-d1ea-40ce-df9b-084ef533fb89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1571800648227,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "yg8P3ve8B-Zb",
    "outputId": "affe5a55-9f01-4315-b656-4c3fc1bcfaec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jmnoxk7RJO9"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade \"tensorflow==1.4\" \"keras>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U-KL2phnPRX"
   },
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
    "X_train_std = np.std(X_train, axis=(0,1,2))\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ggyKUbYm9Zy"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.SeparableConv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.SeparableConv2D(int(num_filter*compression), (5,5), use_bias=False,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(4,4))(relu)\n",
    "    #flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.SeparableConv2D(num_classes, (5,5), use_bias=False,padding='same', activation='softmax')(AvgPooling)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2ZiBEeom9aG"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK5dSBaCm9aM"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade \"tensorflow==1.4\" \"keras>=2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g845JPMCm9aS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "num_filter = 126\n",
    "\n",
    "dropout_rate = 0\n",
    "l = 7\n",
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.SeparableConv2D(num_filter, (5,5), use_bias=False, padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2625,
     "status": "ok",
     "timestamp": 1571804876146,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "k0bjetnMnSb3",
    "outputId": "70a87f94-7cc2-4559-c2cd-1b58d61ed82c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 32, 32, 126)  453         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 126)  504         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 126)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 69)   11844       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 195)  0           separable_conv2d[0][0]           \n",
      "                                                                 separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 195)  780         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 195)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 32, 32, 69)   18330       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 264)  0           concatenate[0][0]                \n",
      "                                                                 separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 264)  1056        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 264)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 32, 32, 69)   24816       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 333)  0           concatenate_1[0][0]              \n",
      "                                                                 separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 333)  1332        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 333)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 32, 32, 69)   31302       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 402)  0           concatenate_2[0][0]              \n",
      "                                                                 separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 402)  1608        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 402)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 32, 32, 69)   37788       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 471)  0           concatenate_3[0][0]              \n",
      "                                                                 separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 471)  1884        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 471)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 32, 32, 69)   44274       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 540)  0           concatenate_4[0][0]              \n",
      "                                                                 separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 540)  2160        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 540)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 32, 32, 69)   50760       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 609)  0           concatenate_5[0][0]              \n",
      "                                                                 separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 609)  2436        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 609)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 32, 32, 69)   57246       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 69)   0           separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 69)   276         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 69)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 16, 16, 69)   6486        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 138)  0           average_pooling2d[0][0]          \n",
      "                                                                 separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 138)  552         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 138)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 16, 16, 69)   12972       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 207)  0           concatenate_7[0][0]              \n",
      "                                                                 separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 207)  828         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 207)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 16, 16, 69)   19458       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 276)  0           concatenate_8[0][0]              \n",
      "                                                                 separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 276)  1104        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 276)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 16, 16, 69)   25944       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 345)  0           concatenate_9[0][0]              \n",
      "                                                                 separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 345)  1380        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 345)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 16, 16, 69)   32430       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 414)  0           concatenate_10[0][0]             \n",
      "                                                                 separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 414)  1656        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 414)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 16, 16, 69)   38916       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 483)  0           concatenate_11[0][0]             \n",
      "                                                                 separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 483)  1932        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 483)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 16, 16, 69)   45402       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 552)  0           concatenate_12[0][0]             \n",
      "                                                                 separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 552)  2208        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 552)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 16, 16, 69)   51888       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 69)     0           separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 69)     276         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 69)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 8, 8, 69)     6486        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 138)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 138)    552         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 138)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 8, 8, 69)     12972       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 8, 8, 207)    0           concatenate_14[0][0]             \n",
      "                                                                 separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 207)    828         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 207)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 8, 8, 69)     19458       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 8, 8, 276)    0           concatenate_15[0][0]             \n",
      "                                                                 separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 276)    1104        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 276)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 8, 8, 69)     25944       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 345)    0           concatenate_16[0][0]             \n",
      "                                                                 separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 345)    1380        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 345)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 8, 8, 69)     32430       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 8, 8, 414)    0           concatenate_17[0][0]             \n",
      "                                                                 separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 414)    1656        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 414)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 8, 8, 69)     38916       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 8, 8, 483)    0           concatenate_18[0][0]             \n",
      "                                                                 separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 483)    1932        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 483)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 8, 8, 69)     45402       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 8, 8, 552)    0           concatenate_19[0][0]             \n",
      "                                                                 separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 552)    2208        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 552)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 8, 8, 69)     51888       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 69)     0           separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 69)     276         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 69)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 4, 4, 69)     6486        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 4, 4, 138)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 138)    552         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 138)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 4, 4, 69)     12972       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 207)    0           concatenate_21[0][0]             \n",
      "                                                                 separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 207)    828         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 207)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 4, 4, 69)     19458       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 276)    0           concatenate_22[0][0]             \n",
      "                                                                 separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 276)    1104        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 276)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 4, 4, 69)     25944       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 4, 4, 345)    0           concatenate_23[0][0]             \n",
      "                                                                 separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 345)    1380        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 345)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 4, 4, 69)     32430       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 4, 4, 414)    0           concatenate_24[0][0]             \n",
      "                                                                 separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 414)    1656        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 414)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 4, 4, 69)     38916       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 4, 4, 483)    0           concatenate_25[0][0]             \n",
      "                                                                 separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 483)    1932        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 483)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 4, 4, 69)     45402       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 4, 4, 552)    0           concatenate_26[0][0]             \n",
      "                                                                 separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 552)    2208        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 552)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 552)    0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 1, 1, 10)     19320       average_pooling2d_3[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 986,301\n",
      "Trainable params: 965,517\n",
      "Non-trainable params: 20,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StpOIZ7oA6cw"
   },
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.10,\n",
    "    )\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4Z3iNkPJLoU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipb_Nt23AN82"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "filepath=\"epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
    "checkpoint_1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ozyhOX_xSqt"
   },
   "outputs": [],
   "source": [
    "reduce_lr_1 = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=4,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckua1H9TC2wA"
   },
   "outputs": [],
   "source": [
    "earlystopping_1 = EarlyStopping(monitor='val_loss', patience=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3lFweREIKuS"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [earlystopping_1,reduce_lr_1,checkpoint_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping to match with convoultion output layer\n",
    "y_train_re = np.reshape(y_train, (50000,1,1,10))\n",
    "y_test_re = np.reshape(y_test, (10000,1,1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1507390,
     "status": "ok",
     "timestamp": 1571644199025,
     "user": {
      "displayName": "sandeep Burra",
      "photoUrl": "",
      "userId": "00094485018250843945"
     },
     "user_tz": -330
    },
    "id": "crhGk7kEhXAz",
    "outputId": "acafc394-e8b9-49b4-aa8c-0276f608af2f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.6015 - acc: 0.4069\n",
      "Epoch 00001: saving model to epochs:001-val_acc:0.509.hdf5\n",
      "781/781 [==============================] - 142s 182ms/step - loss: 1.6011 - acc: 0.4070 - val_loss: 1.4417 - val_acc: 0.5087\n",
      "Epoch 2/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 1.1707 - acc: 0.5825\n",
      "Epoch 00002: saving model to epochs:002-val_acc:0.530.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 1.1703 - acc: 0.5827 - val_loss: 1.5555 - val_acc: 0.5298\n",
      "Epoch 3/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.9674 - acc: 0.6601\n",
      "Epoch 00003: saving model to epochs:003-val_acc:0.590.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.9678 - acc: 0.6600 - val_loss: 1.3740 - val_acc: 0.5901\n",
      "Epoch 4/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7098\n",
      "Epoch 00004: saving model to epochs:004-val_acc:0.704.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8377 - acc: 0.7098 - val_loss: 0.8627 - val_acc: 0.7037\n",
      "Epoch 5/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7396\n",
      "Epoch 00005: saving model to epochs:005-val_acc:0.717.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.7500 - acc: 0.7395 - val_loss: 0.8827 - val_acc: 0.7166\n",
      "Epoch 6/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6839 - acc: 0.7615\n",
      "Epoch 00006: saving model to epochs:006-val_acc:0.784.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.6837 - acc: 0.7617 - val_loss: 0.6355 - val_acc: 0.7838\n",
      "Epoch 7/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.7787\n",
      "Epoch 00007: saving model to epochs:007-val_acc:0.772.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.6391 - acc: 0.7787 - val_loss: 0.6830 - val_acc: 0.7720\n",
      "Epoch 8/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.7930\n",
      "Epoch 00008: saving model to epochs:008-val_acc:0.791.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.5996 - acc: 0.7929 - val_loss: 0.6126 - val_acc: 0.7910\n",
      "Epoch 9/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.8089\n",
      "Epoch 00009: saving model to epochs:009-val_acc:0.771.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.5590 - acc: 0.8088 - val_loss: 0.6974 - val_acc: 0.7709\n",
      "Epoch 10/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8171\n",
      "Epoch 00010: saving model to epochs:010-val_acc:0.824.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.5265 - acc: 0.8171 - val_loss: 0.5226 - val_acc: 0.8243\n",
      "Epoch 11/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.8252\n",
      "Epoch 00011: saving model to epochs:011-val_acc:0.807.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.4996 - acc: 0.8253 - val_loss: 0.5991 - val_acc: 0.8068\n",
      "Epoch 12/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4670 - acc: 0.8384\n",
      "Epoch 00012: saving model to epochs:012-val_acc:0.835.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.4672 - acc: 0.8384 - val_loss: 0.5170 - val_acc: 0.8345\n",
      "Epoch 13/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8452\n",
      "Epoch 00013: saving model to epochs:013-val_acc:0.837.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.4477 - acc: 0.8451 - val_loss: 0.4697 - val_acc: 0.8374\n",
      "Epoch 14/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8511\n",
      "Epoch 00014: saving model to epochs:014-val_acc:0.839.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.4282 - acc: 0.8512 - val_loss: 0.4808 - val_acc: 0.8393\n",
      "Epoch 15/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8567\n",
      "Epoch 00015: saving model to epochs:015-val_acc:0.853.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.4078 - acc: 0.8568 - val_loss: 0.4367 - val_acc: 0.8533\n",
      "Epoch 16/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8643\n",
      "Epoch 00016: saving model to epochs:016-val_acc:0.846.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.3957 - acc: 0.8643 - val_loss: 0.4704 - val_acc: 0.8462\n",
      "Epoch 17/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8663\n",
      "Epoch 00017: saving model to epochs:017-val_acc:0.853.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.3832 - acc: 0.8664 - val_loss: 0.4462 - val_acc: 0.8528\n",
      "Epoch 18/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3667 - acc: 0.8735\n",
      "Epoch 00018: saving model to epochs:018-val_acc:0.849.hdf5\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.3668 - acc: 0.8735 - val_loss: 0.4600 - val_acc: 0.8485\n",
      "Epoch 19/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8770\n",
      "Epoch 00019: saving model to epochs:019-val_acc:0.866.hdf5\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.3537 - acc: 0.8770 - val_loss: 0.3959 - val_acc: 0.8656\n",
      "Epoch 20/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8835\n",
      "Epoch 00020: saving model to epochs:020-val_acc:0.852.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.3347 - acc: 0.8835 - val_loss: 0.4636 - val_acc: 0.8523\n",
      "Epoch 21/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8865\n",
      "Epoch 00021: saving model to epochs:021-val_acc:0.877.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.3247 - acc: 0.8865 - val_loss: 0.3769 - val_acc: 0.8768\n",
      "Epoch 22/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8886\n",
      "Epoch 00022: saving model to epochs:022-val_acc:0.871.hdf5\n",
      "781/781 [==============================] - 129s 165ms/step - loss: 0.3182 - acc: 0.8886 - val_loss: 0.4012 - val_acc: 0.8708\n",
      "Epoch 23/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.8914\n",
      "Epoch 00023: saving model to epochs:023-val_acc:0.852.hdf5\n",
      "781/781 [==============================] - 130s 167ms/step - loss: 0.3067 - acc: 0.8914 - val_loss: 0.4906 - val_acc: 0.8523\n",
      "Epoch 24/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.8959\n",
      "Epoch 00024: saving model to epochs:024-val_acc:0.864.hdf5\n",
      "781/781 [==============================] - 131s 167ms/step - loss: 0.2997 - acc: 0.8958 - val_loss: 0.4362 - val_acc: 0.8643\n",
      "Epoch 25/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.8986\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00025: saving model to epochs:025-val_acc:0.854.hdf5\n",
      "781/781 [==============================] - 131s 168ms/step - loss: 0.2904 - acc: 0.8986 - val_loss: 0.4713 - val_acc: 0.8538\n",
      "Epoch 26/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9256\n",
      "Epoch 00026: saving model to epochs:026-val_acc:0.899.hdf5\n",
      "781/781 [==============================] - 131s 167ms/step - loss: 0.2167 - acc: 0.9255 - val_loss: 0.3181 - val_acc: 0.8991\n",
      "Epoch 27/100\n",
      " 31/781 [>.............................] - ETA: 1:57 - loss: 0.2121 - acc: 0.9315"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, y_train_re, batch_size=batch_size),\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                    epochs=100,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test_re),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection got disconnected, so resuminf the traimimg process from epoch27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"epochs:026-val_acc:0.899.hdf5\" \n",
    "model = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9328\n",
      "Epoch 00027: saving model to epochs:027-val_acc:0.902.hdf5\n",
      "781/781 [==============================] - 144s 184ms/step - loss: 0.1913 - acc: 0.9328 - val_loss: 0.3185 - val_acc: 0.9017\n",
      "Epoch 28/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9379\n",
      "Epoch 00028: saving model to epochs:028-val_acc:0.902.hdf5\n",
      "781/781 [==============================] - 127s 162ms/step - loss: 0.1795 - acc: 0.9379 - val_loss: 0.3165 - val_acc: 0.9021\n",
      "Epoch 29/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9392\n",
      "Epoch 00029: saving model to epochs:029-val_acc:0.905.hdf5\n",
      "781/781 [==============================] - 127s 162ms/step - loss: 0.1752 - acc: 0.9391 - val_loss: 0.3142 - val_acc: 0.9052\n",
      "Epoch 30/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9407\n",
      "Epoch 00030: saving model to epochs:030-val_acc:0.904.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1695 - acc: 0.9407 - val_loss: 0.3111 - val_acc: 0.9044\n",
      "Epoch 31/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9429\n",
      "Epoch 00031: saving model to epochs:031-val_acc:0.904.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1640 - acc: 0.9429 - val_loss: 0.3155 - val_acc: 0.9040\n",
      "Epoch 32/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9453\n",
      "Epoch 00032: saving model to epochs:032-val_acc:0.903.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1590 - acc: 0.9453 - val_loss: 0.3259 - val_acc: 0.9031\n",
      "Epoch 33/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9444\n",
      "Epoch 00033: saving model to epochs:033-val_acc:0.904.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1580 - acc: 0.9443 - val_loss: 0.3252 - val_acc: 0.9042\n",
      "Epoch 34/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9469\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00034: saving model to epochs:034-val_acc:0.906.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.1499 - acc: 0.9470 - val_loss: 0.3187 - val_acc: 0.9057\n",
      "Epoch 35/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9482\n",
      "Epoch 00035: saving model to epochs:035-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1458 - acc: 0.9482 - val_loss: 0.3098 - val_acc: 0.9075\n",
      "Epoch 36/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9498\n",
      "Epoch 00036: saving model to epochs:036-val_acc:0.907.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1462 - acc: 0.9498 - val_loss: 0.3088 - val_acc: 0.9071\n",
      "Epoch 37/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9495\n",
      "Epoch 00037: saving model to epochs:037-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1435 - acc: 0.9495 - val_loss: 0.3077 - val_acc: 0.9083\n",
      "Epoch 38/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9499\n",
      "Epoch 00038: saving model to epochs:038-val_acc:0.907.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1417 - acc: 0.9499 - val_loss: 0.3097 - val_acc: 0.9074\n",
      "Epoch 39/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9510\n",
      "Epoch 00039: saving model to epochs:039-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1413 - acc: 0.9510 - val_loss: 0.3077 - val_acc: 0.9086\n",
      "Epoch 40/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9500\n",
      "Epoch 00040: saving model to epochs:040-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1413 - acc: 0.9501 - val_loss: 0.3082 - val_acc: 0.9081\n",
      "Epoch 41/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9513\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00041: saving model to epochs:041-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1389 - acc: 0.9513 - val_loss: 0.3102 - val_acc: 0.9087\n",
      "Epoch 42/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9501\n",
      "Epoch 00042: saving model to epochs:042-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1417 - acc: 0.9501 - val_loss: 0.3119 - val_acc: 0.9085\n",
      "Epoch 43/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9514\n",
      "Epoch 00043: saving model to epochs:043-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1395 - acc: 0.9515 - val_loss: 0.3106 - val_acc: 0.9082\n",
      "Epoch 44/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9507\n",
      "Epoch 00044: saving model to epochs:044-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1402 - acc: 0.9507 - val_loss: 0.3099 - val_acc: 0.9092\n",
      "Epoch 45/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9534\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00045: saving model to epochs:045-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1336 - acc: 0.9534 - val_loss: 0.3104 - val_acc: 0.9089\n",
      "Epoch 46/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9532\n",
      "Epoch 00046: saving model to epochs:046-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1370 - acc: 0.9532 - val_loss: 0.3099 - val_acc: 0.9081\n",
      "Epoch 47/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9515\n",
      "Epoch 00047: saving model to epochs:047-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1387 - acc: 0.9515 - val_loss: 0.3096 - val_acc: 0.9090\n",
      "Epoch 48/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9534\n",
      "Epoch 00048: saving model to epochs:048-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1379 - acc: 0.9534 - val_loss: 0.3106 - val_acc: 0.9087\n",
      "Epoch 49/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9512\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00049: saving model to epochs:049-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1413 - acc: 0.9512 - val_loss: 0.3091 - val_acc: 0.9087\n",
      "Epoch 50/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9514\n",
      "Epoch 00050: saving model to epochs:050-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1388 - acc: 0.9514 - val_loss: 0.3107 - val_acc: 0.9088\n",
      "Epoch 51/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9536\n",
      "Epoch 00051: saving model to epochs:051-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1358 - acc: 0.9535 - val_loss: 0.3088 - val_acc: 0.9086\n",
      "Epoch 52/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9511\n",
      "Epoch 00052: saving model to epochs:052-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1398 - acc: 0.9511 - val_loss: 0.3092 - val_acc: 0.9083\n",
      "Epoch 53/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9531\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 00053: saving model to epochs:053-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1359 - acc: 0.9531 - val_loss: 0.3096 - val_acc: 0.9081\n",
      "Epoch 54/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9534\n",
      "Epoch 00054: saving model to epochs:054-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1365 - acc: 0.9535 - val_loss: 0.3101 - val_acc: 0.9087\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/781 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9522\n",
      "Epoch 00055: saving model to epochs:055-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1367 - acc: 0.9522 - val_loss: 0.3102 - val_acc: 0.9082\n",
      "Epoch 56/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9522\n",
      "Epoch 00056: saving model to epochs:056-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1368 - acc: 0.9522 - val_loss: 0.3088 - val_acc: 0.9091\n",
      "Epoch 57/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9527\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 00057: saving model to epochs:057-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1347 - acc: 0.9527 - val_loss: 0.3091 - val_acc: 0.9090\n",
      "Epoch 58/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9536\n",
      "Epoch 00058: saving model to epochs:058-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1370 - acc: 0.9536 - val_loss: 0.3102 - val_acc: 0.9081\n",
      "Epoch 59/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9521\n",
      "Epoch 00059: saving model to epochs:059-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1372 - acc: 0.9521 - val_loss: 0.3118 - val_acc: 0.9082\n",
      "Epoch 60/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9510\n",
      "Epoch 00060: saving model to epochs:060-val_acc:0.910.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1378 - acc: 0.9510 - val_loss: 0.3090 - val_acc: 0.9095\n",
      "Epoch 61/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9533\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 00061: saving model to epochs:061-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1373 - acc: 0.9533 - val_loss: 0.3096 - val_acc: 0.9085\n",
      "Epoch 62/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9534\n",
      "Epoch 00062: saving model to epochs:062-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1383 - acc: 0.9534 - val_loss: 0.3094 - val_acc: 0.9086\n",
      "Epoch 63/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9526\n",
      "Epoch 00063: saving model to epochs:063-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1366 - acc: 0.9526 - val_loss: 0.3107 - val_acc: 0.9088\n",
      "Epoch 64/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9521\n",
      "Epoch 00064: saving model to epochs:064-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 131s 168ms/step - loss: 0.1353 - acc: 0.9521 - val_loss: 0.3099 - val_acc: 0.9088\n",
      "Epoch 65/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9512\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "\n",
      "Epoch 00065: saving model to epochs:065-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.1393 - acc: 0.9512 - val_loss: 0.3093 - val_acc: 0.9090\n",
      "Epoch 66/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9520\n",
      "Epoch 00066: saving model to epochs:066-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.1357 - acc: 0.9520 - val_loss: 0.3098 - val_acc: 0.9091\n",
      "Epoch 67/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9537\n",
      "Epoch 00067: saving model to epochs:067-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1336 - acc: 0.9536 - val_loss: 0.3110 - val_acc: 0.9088\n",
      "Epoch 68/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9523\n",
      "Epoch 00068: saving model to epochs:068-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1361 - acc: 0.9524 - val_loss: 0.3095 - val_acc: 0.9088\n",
      "Epoch 69/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9506\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "\n",
      "Epoch 00069: saving model to epochs:069-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1437 - acc: 0.9506 - val_loss: 0.3105 - val_acc: 0.9081\n",
      "Epoch 70/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9524\n",
      "Epoch 00070: saving model to epochs:070-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1370 - acc: 0.9524 - val_loss: 0.3086 - val_acc: 0.9087\n",
      "Epoch 71/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9518\n",
      "Epoch 00071: saving model to epochs:071-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1394 - acc: 0.9518 - val_loss: 0.3087 - val_acc: 0.9087\n",
      "Epoch 72/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9515\n",
      "Epoch 00072: saving model to epochs:072-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1378 - acc: 0.9515 - val_loss: 0.3105 - val_acc: 0.9081\n",
      "Epoch 73/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9511\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "\n",
      "Epoch 00073: saving model to epochs:073-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 128s 164ms/step - loss: 0.1384 - acc: 0.9511 - val_loss: 0.3133 - val_acc: 0.9079\n",
      "Epoch 74/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9505\n",
      "Epoch 00074: saving model to epochs:074-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1405 - acc: 0.9505 - val_loss: 0.3097 - val_acc: 0.9091\n",
      "Epoch 75/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9522\n",
      "Epoch 00075: saving model to epochs:075-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1376 - acc: 0.9522 - val_loss: 0.3110 - val_acc: 0.9084\n",
      "Epoch 76/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9520\n",
      "Epoch 00076: saving model to epochs:076-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1389 - acc: 0.9520 - val_loss: 0.3100 - val_acc: 0.9086\n",
      "Epoch 77/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9524\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "\n",
      "Epoch 00077: saving model to epochs:077-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1382 - acc: 0.9524 - val_loss: 0.3106 - val_acc: 0.9090\n",
      "Epoch 78/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9519\n",
      "Epoch 00078: saving model to epochs:078-val_acc:0.908.hdf5\n",
      "781/781 [==============================] - 128s 163ms/step - loss: 0.1396 - acc: 0.9519 - val_loss: 0.3105 - val_acc: 0.9083\n",
      "Epoch 79/100\n",
      "780/781 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9512\n",
      "Epoch 00079: saving model to epochs:079-val_acc:0.909.hdf5\n",
      "781/781 [==============================] - 127s 163ms/step - loss: 0.1404 - acc: 0.9513 - val_loss: 0.3116 - val_acc: 0.9088\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6c6819fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(X_train, y_train_re, batch_size=batch_size),\n",
    "                    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                    epochs=100,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test_re),callbacks=callbacks_list, initial_epoch=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcWydmIVhZGr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 652us/sample - loss: 0.3113 - acc: 0.9088\n",
      "Test loss: 0.31126459009647367\n",
      "Test accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test_re, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE3lF6EH1r_L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of DenseNet - cifar10.ipynb",
   "provenance": [
    {
     "file_id": "16-fwEHK6Pl9j_Kt546WNb2wx9Z_ky6W-",
     "timestamp": 1566707289180
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
